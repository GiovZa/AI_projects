{"cells":[{"cell_type":"markdown","metadata":{"id":"Gd7scc-8QJvE"},"source":["## CS441: Applied ML - HW 3"]},{"cell_type":"markdown","metadata":{"id":"QagOldZDQJvG"},"source":["## Part 1: Spam Detection with Naive Bayes Classifier\n","\n","We want to classify text messages as “spam” (unwanted) or “ham” (genuine). We will use data (spam.csv) from the Kaggle SMS spam dataset. We’ve provided the loading and pre-processing code to generate:\n","* `unique_words`: the unique set of words in the dataset\n","* `(x_train, y_train, msg_train)`: counts of words in each message, spam (y=1) or not spam (y=-1) labels, and the message string for each training sample; `x_train[n][j]` is the count of the `j`th word in the `n`th sample.\n","* `*_val` and `*_test`, similar to above, for the val and test splits\n","We will use a Naive Bayes Classifier.\n","\n","See assignment for details (equations are not easy to reproduce here)."]},{"cell_type":"code","source":["import csv\n","import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","datadir = \"/content/drive/My Drive/CS441/24FA/hw3/\" # choose proper location\n","\n","# don't change the code below\n","\n","# read data\n","with open(datadir + 'spam.csv', encoding='latin-1') as csvfile:\n","    datareader = csv.reader(csvfile, delimiter=',')\n","    y = np.zeros((10000,))\n","    X = []\n","    n = 0\n","    rownum = 0\n","    for row in datareader:\n","      if rownum == 0:\n","        rownum += 1\n","        continue\n","      rownum += 1\n","      if row[0]=='ham':\n","        y[n] = -1\n","      else:\n","        y[n] = 1\n","      X.append(row[1])\n","      n += 1\n","y= y[:n]\n","\n","# y[n] = -1 for ham, 1 for spam\n","print(y[0])\n","print(X[0])\n","\n","# parse the text messages into words and count the words in each row\n","vectorizer = CountVectorizer(analyzer='word')\n","word_count = vectorizer.fit_transform(X).toarray()\n","\n","print(f\"We have {word_count.shape[0]} examples with {word_count.shape[1]} unique words.\")\n","unique_words = vectorizer.get_feature_names_out()\n","\n","# split data into train (50%), validation (25%), and test (25%)\n","x_train = word_count[::2]\n","y_train = y[::2]\n","msg_train = X[::2]\n","x_val = word_count[1::4]\n","y_val = y[1::4]\n","msg_val = X[1::4]\n","x_test = word_count[3::4]\n","y_test = y[3::4]\n","msg_test = X[3::4]\n"],"metadata":{"id":"EBzNRkN2AmWA","executionInfo":{"status":"ok","timestamp":1729395163816,"user_tz":300,"elapsed":6555,"user":{"displayName":"Gio Zavalza","userId":"17675791871333725061"}},"outputId":"bceebc00-6355-4175-ab20-f46ae0f3fb54","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","-1.0\n","Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n","We have 5572 examples with 8672 unique words.\n"]}]},{"cell_type":"markdown","source":["#### 1. Train your Naive Bayes Classifier\n","I.e. P(y), P(w|y)) using the train set with =1 and compute the accuracy on the val set.  You should get P(y=1)=0.142, P(call|spam) = 0.0104, and P(call|ham)=0.0029. Your validation accuracy should be higher than 95%."],"metadata":{"id":"pTZ05VOmAshI"}},{"cell_type":"code","source":["# TO DO\n","\n","# Compute the priors spam and ham\n","Prob_spam = np.mean(y_train == 1)\n","Prob_ham = np.mean(y_train == -1)\n","\n","print(\"You should get\")\n","print(f\"P(y=1)={Prob_spam:.3f},\")\n","\n","''' Check if spam + ham == 1 '''\n","# print(f\"P(y=-1)={Prob_ham:.3f},\")\n","# print(f\"1 == {Prob_ham + Prob_spam:.3f}\")\n","\n","# Get sums and laplace for likelihood calculations\n","alpha = 1  # 1 is default value for laplace\n","num_spam_words = np.sum(x_train[y_train == 1], axis=0)\n","num_ham_words = np.sum(x_train[y_train == -1], axis=0)\n","total_words_spam = np.sum(num_spam_words)\n","total_words_ham = np.sum(num_ham_words)\n","\n","# Compute the likelihoods using Laplace smoothing\n","P_word_given_spam = (num_spam_words + alpha) / (total_words_spam + alpha * len(unique_words)) # Laplace to not break calculation when total words = 0\n","P_word_given_ham = (num_ham_words + alpha) / (total_words_ham + alpha * len(unique_words))\n","\n","# Find givens for the word call\n","word_idx = np.where(unique_words == 'call')[0]\n","print(f\"P(call|spam) = {P_word_given_spam[word_idx[0]]:.4f}, and\")\n","print(f\"P(call|ham) = {P_word_given_ham[word_idx[0]]:.4f}\")\n","\n","# Predict and calculate accuracy in the validation set\n","log_spam_prob = np.log(Prob_spam) + x_val.dot(np.log(P_word_given_spam)) # Log to prevent underflow\n","log_ham_prob = np.log(Prob_ham) + x_val.dot(np.log(P_word_given_ham))\n","spam_scores = log_spam_prob - log_ham_prob\n","y_val_pred = np.where(spam_scores > 0, 1, -1)\n","\n","# Check if validation accuracy is > 95%\n","print(\"Your validation accuracy should be higher than 95%\")\n","accuracy = np.mean(y_val_pred == y_val)\n","print(f\"Validation Accuracy: {accuracy:.2%}\")\n"],"metadata":{"id":"nVa3s_iYAwkv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729395164136,"user_tz":300,"elapsed":323,"user":{"displayName":"Gio Zavalza","userId":"17675791871333725061"}},"outputId":"c89b3b05-06cc-491e-8a32-2f3c33b6fcc9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["You should get\n","P(y=1)=0.142,\n","P(call|spam) = 0.0104, and\n","P(call|ham) = 0.0029\n","Your validation accuracy should be higher than 95%\n","Validation Accuracy: 98.13%\n"]}]},{"cell_type":"markdown","source":["#### 2. Data exploration\n","What are the 10 spammiest words (i.e. words with highest  logP(wj|spam)- logP(wj|ham))?  What are the 10 hammiest words? Which val message is the spammiest ham (message with highest spam score but y=-1)? Which is hammiest spam (message with lowest spam score but y=1)?  Spammiest spam? Hammiest ham?"],"metadata":{"id":"me2ooMJ8A1qw"}},{"cell_type":"code","source":["# TO DO\n","\n","# Compute the difference in log-probabilities\n","log_diff = np.log(P_word_given_spam) - np.log(P_word_given_ham)\n","\n","# 10 spammiest words, the 10 hammiest words\n","spammiest_words = unique_words[np.argsort(log_diff)[-10:]]\n","hammiest_words = unique_words[np.argsort(log_diff)[:10]]\n","print(\"Spammiest Words:\", spammiest_words)\n","print(\"Hammiest Words:\", hammiest_words)\n","\n","# The spammiest ham\n","ham_messages = np.array(msg_val)[y_val == -1] # Get booleans of each word's spamness\n","spammiest_ham_index = np.argmax(spam_scores[y_val == -1])  # Get the index with the most spam score  but label is 'ham'\n","spammiest_ham_msg = ham_messages[spammiest_ham_index]  # Get the ham msg using the index given\n","print(f\"The Spammiest Ham: {spammiest_ham_msg}\")\n","\n","# The hammiest spam\n","spam_messages = np.array(msg_val)[y_val == 1] # Get booleans of each word's spamness\n","hammiest_spam_index = np.argmin(spam_scores[y_val == 1])  # Get the index with the least spam score but label is 'spam'\n","hammiest_spam_msg = spam_messages[hammiest_spam_index]  # Get the spam msg using the index given\n","print(f\"Hammiest Spam: {hammiest_spam_msg}\")\n","\n","# The spammiest spam\n","spammiest_spam_index = np.argmax(spam_scores[y_val == 1]) # same as hammiest spam but get argmax\n","spammiest_spam_msg = spam_messages[spammiest_spam_index]\n","\n","# The hammiest ham\n","hammiest_ham_index = np.argmin(spam_scores[y_val == -1]) # same as spammiest ham but get argmin\n","hammiest_ham_msg = ham_messages[hammiest_ham_index]\n","\n","print(f\"Spammiest Spam: {spammiest_spam_msg}\")\n","print(f\"Hammiest Ham: {hammiest_ham_msg}\")\n"],"metadata":{"id":"i2b_U4qbIa1D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729395164137,"user_tz":300,"elapsed":12,"user":{"displayName":"Gio Zavalza","userId":"17675791871333725061"}},"outputId":"b088f4f9-e8e6-4479-9aa2-20b08fc523f4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Spammiest Words: ['18' 'cs' '16' '500' 'tone' 'www' '150p' 'uk' 'prize' 'claim']\n","Hammiest Words: ['gt' 'lt' 'he' 'but' 'lor' 'da' 'she' 'later' 'ì_' 'wat']\n","The Spammiest Ham: Waqt se pehle or naseeb se zyada kisi ko kuch nahi milta,Zindgi wo nahi he jo hum sochte hai Zindgi wo hai jo ham jeetey hai..........\n","Hammiest Spam: LIFE has never been this much fun and great until you came in. You made it truly special for me. I won't forget you! enjoy @ one gbp/sms\n","Spammiest Spam: FREE for 1st week! No1 Nokia tone 4 ur mob every week just txt NOKIA to 8007 Get txting and tell ur mates www.getzed.co.uk POBox 36504 W45WQ norm150p/tone 16+\n","Hammiest Ham: Sad story of a Man - Last week was my b'day. My Wife did'nt wish me. My Parents forgot n so did my Kids . I went to work. Even my Colleagues did not wish. As I entered my cabin my PA said, '' Happy B'day Boss !!''. I felt special. She askd me 4 lunch. After lunch she invited me to her apartment. We went there. She said,'' do u mind if I go into the bedroom for a minute ? '' ''OK'', I sed in a sexy mood. She came out 5 minuts latr wid a cake...n My Wife, My Parents, My Kidz, My Friends n My Colleagues. All screaming.. SURPRISE !! and I was waiting on the sofa.. ... ..... ' NAKED...!\n"]}]},{"cell_type":"markdown","source":["#### 3. Precision-recall trade-off\n","You want to flag spam messages with minimal false positives. Using the val set, compute precision/recall and display the PR curve. Programmatically, find the threshold with highest recall, where precision > 0.99.  Report the accuracy, precision, and recall on the test set using the same model and the selected threshold."],"metadata":{"id":"B04qPuOaBIlU"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","# https://stackoverflow.com/questions/31421413/how-to-compute-precision-recall-accuracy-and-f1-score-for-the-multiclass-case\n","from sklearn.metrics import precision_recall_curve, accuracy_score, precision_score, recall_score\n","\n","\n","# TO DO\n","\n","# Use the provided p_r_curve function to get needed variables for graph\n","precision, recall, thresholds = precision_recall_curve(y_val, spam_scores)\n","\n","plt.figure()\n","plt.plot(recall, precision)\n","plt.xlabel(\"Recall\")\n","plt.ylabel(\"Precision\")\n","plt.title(\"Precision-Recall Curve\")\n","plt.show()\n","\n","# Find threshold with highest recall given precision > 0.99\n","sorted_indices = np.argsort(recall[:-1])[::-1]  # Sort recall (invert to get highest to lowest)\n","\n","best_threshold = None\n","best_recall = 0\n","\n","for idx in sorted_indices: # Used argsort to get indices of matching precision\n","    if precision[idx] > 0.99:\n","        best_recall = recall[idx]\n","        best_threshold = thresholds[idx]\n","        break  # The first threshold will be the best\n","\n","print(f\"Best Threshold: {best_threshold}, Precision at threshold: {precision[idx]:.3f}, Recall at threshold: {best_recall:.3f}\")\n","\n","# Use best threshold on test set\n","spam_scores_test = np.log(Prob_spam) + x_test.dot(np.log(P_word_given_spam))\n","ham_scores_test = np.log(Prob_ham) + x_test.dot(np.log(P_word_given_ham))\n","test_spam_scores = spam_scores_test - ham_scores_test\n","\n","# Use where to predict\n","y_test_pred = np.where(test_spam_scores > best_threshold, 1, -1)\n","\n","# Get a, p, and r on the test set\n","accuracy = accuracy_score(y_test, y_test_pred)\n","precision = precision_score(y_test, y_test_pred)\n","recall = recall_score(y_test, y_test_pred)\n","\n","print(f\"Test Set with best threshold Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}\")"],"metadata":{"id":"sjbgB-LHBI9y","colab":{"base_uri":"https://localhost:8080/","height":507},"executionInfo":{"status":"ok","timestamp":1729395164967,"user_tz":300,"elapsed":836,"user":{"displayName":"Gio Zavalza","userId":"17675791871333725061"}},"outputId":"7334580b-ebd9-45ab-8fb3-7f44fcb300cf"},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Best Threshold: 6.095247027449673, Precision at threshold: 0.994, Recall at threshold: 0.867\n","Test Set with best threshold Accuracy: 0.976, Precision: 1.000, Recall: 0.808\n"]}]},{"cell_type":"markdown","source":["## Part 2: Robust Estimation\n","\n","The corrupted salary dataset has three variables: salary, years, school.  Salary is the reported salary of each person.  Years is the number of years of experience in the job.  School is the university where the person last had a degree. For the core assignment, we’ll only use salary, and the stretch goals will use the other two variables. Some of the reported salary information is wrong (some incorrect value is provided), so we want to learn things from the data in a way that is robust to the wrong data. We refer to correctly entered data as “valid”.\n","\n","Estimate the true mean, standard deviation, min, and max of the salaries using three different methods."],"metadata":{"id":"YK769ZQ4NgUP"}},{"cell_type":"code","source":["import numpy as np\n","from matplotlib import pyplot as plt\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","datadir = \"/content/drive/My Drive/CS441/24FA/hw3/\"\n","\n","# load data\n","T = np.load(datadir + 'salary.npz')\n","(salary, years, school) = (T['salary'], T['years'], T['school'])"],"metadata":{"id":"Dzb9-YeTnrie","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729395166043,"user_tz":300,"elapsed":1082,"user":{"displayName":"Gio Zavalza","userId":"17675791871333725061"}},"outputId":"fd4d5de3-b609-4ad8-861a-57515db7465d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["#### 1. Assume no noise\n","Compute the statistics for the data as a whole"],"metadata":{"id":"zKusZuTmqwIE"}},{"cell_type":"code","source":["# TO DO\n","\n","# Pretty straightforward\n","salary_mu = np.mean(salary)\n","salary_std = np.std(salary)\n","salary_min = np.min(salary)\n","salary_max = np.max(salary)\n","\n","print('Mean: {}  Std: {}  Min: {}   Max: {}'.format(salary_mu, salary_std, salary_min, salary_max))"],"metadata":{"id":"U8s5LwbpqaR0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729395166043,"user_tz":300,"elapsed":10,"user":{"displayName":"Gio Zavalza","userId":"17675791871333725061"}},"outputId":"fdf0d702-40e0-4d4f-ad57-c2b779fd17a2"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean: 123749.835  Std: 61953.77348723623  Min: 64694.0   Max: 611494.0\n"]}]},{"cell_type":"markdown","source":["#### 2. Percentiles\n","Assume valid data will fall between the 5th and 95th percentile. Adjust estimates of the min and max by assuming that the valid data has a uniform distribution (see lecture on robust fitting)."],"metadata":{"id":"8bKArxlbvOvW"}},{"cell_type":"code","source":["pct = 0.05\n","\n","# TO DO\n","\n","# https://stackoverflow.com/questions/2374640/how-do-i-calculate-percentiles-with-python-numpy\n","# https://www.geeksforgeeks.org/numpy-percentile-in-python/\n","\n","pct_x_5 = np.percentile(salary, 5)\n","pct_x_95 = np.percentile(salary, 95)\n","\n","# Apply robust fitting\n","salary_min = pct_x_5 - (pct_x_95 - pct_x_5) * (pct / (1 - 2 * pct))\n","salary_max = pct_x_95 + (pct_x_95 - pct_x_5) * (pct / (1 - 2 * pct))\n","\n","# Get rid of data in lower 5 percentile and higher 95 percentile\n","filtered_salary = np.array([])\n","for sal in salary:\n","  if sal > pct_x_5 and sal < pct_x_95:\n","    filtered_salary = np.append(filtered_salary, sal)\n","\n","# Now do the same as before\n","salary_mu = np.mean(filtered_salary)\n","salary_std = np.std(filtered_salary)\n","\n","print('Mean: {}  Std: {}  Min: {}   Max: {}'.format(salary_mu, salary_std, salary_min, salary_max))"],"metadata":{"id":"OVXCYlx7wGjS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729395166043,"user_tz":300,"elapsed":8,"user":{"displayName":"Gio Zavalza","userId":"17675791871333725061"}},"outputId":"46a7bdbc-3ba1-4e2a-b47f-7f59e94aaf16"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean: 113878.65  Std: 15876.450453939286  Min: 75493.8   Max: 159900.79999999973\n"]}]},{"cell_type":"markdown","source":["#### 3. EM\n","Assume valid data follows a Gaussian distribution, while the fake data has a uniform distribution between the minimum and maximum value of salary. For mean and std, report the estimated mean and std of the valid salary distribution. For min and max, report the min and max salaries that have greater than 50% chance of being valid. Also report the estimated probability that a random sample is valid, and the first five indices of salaries that are not likely to be valid."],"metadata":{"id":"fdQCUF2aq-9p"}},{"cell_type":"code","source":["\n","# TO DO\n","\n","# Take example from EM - Annotator Problem,\n","# but revise to have M (Num of Annotators) be 1 since each datapoint is 1 teacher\n","# and we do not know the true salary (Which is when we should be using EM Algo)\n","\n","# We can not plot because the true salary is unknown\n","# def plot_est(true_salaries, est_salaries):\n","#   est_salaries = est_salaries.flatten()\n","#   msq = 0\n","#   for i in range(len(true_salaries)):\n","#     msq += (true_salaries[i]-est_salaries[i])**2\n","#     plt.plot(i, true_salaries[i], 'g.')\n","#     plt.plot(i, est_salaries[i], 'r*')\n","#     plt.plot([i, i], [true_salaries[i], est_salaries[i]], 'r')\n","#   plt.show()\n","#   print('RMS Err: {:0.3f}'.format(np.sqrt(msq/len(true_salaries))))\n","\n","print(\"Salary shape:\", salary.shape)\n","\n","\n","# Initialize parameters\n","niter = 20\n","N = salary.shape[0]\n","M = 1 # N is 1-dimensional\n","\n","# Initialize by assuming that all salaries are good\n","salary_mean = salary.mean()  # mu_i\n","salary_std = np.sqrt(np.sum((salary - salary_mean) ** 2, axis=None) / N / M)  # sigma\n","pz = 0.5  # P(z=1) = 0.5 initially\n","\n","# Plot initial estimate // No plotting because we do not know true data\n","# true_salary = 10 * (np.sin(np.arange(N) / N * 2 * np.pi) + 1.2) / 2.4\n","# plot_est(true_salary, np.full_like(true_salary, salary_mean))  # Plot constant mean\n","\n","for t in range(niter):\n","\n","  last_mean = salary_mean.copy()\n","\n","  # E-step\n","  # update probability that each annotator is good\n","  p_valid_given_s = np.zeros((N,1))  # w_a = P(z_a=1 | salarys, theta_t)\n","\n","  for i in range(N):\n","    p_s_good = pz * (1 / (np.sqrt(2 * np.pi) * salary_std)) * np.exp(-0.5 * ((salary[i] - salary_mean) / salary_std) ** 2)\n","    p_s_bad = (1 - pz) * (1 / (salary.max() - salary.min()))  # Uniform probability for invalid data\n","    p_valid_given_s[i] = p_s_good / (p_s_good + p_s_bad)\n","  print('\\niter {}'.format(t))\n","  print(np.round(p_valid_given_s.transpose()*1000)/1000)\n","\n","  # M-step\n","  # assign parameters that maximize likelihood under latent variable likelihoods\n","\n","  # estimate mean\n","  w_salary_sum = 0\n","  for i in range(N): # Do not need to double loop since N is 1-D\n","    w_salary_sum += salary[i] * p_valid_given_s[i]\n","  salary_mean = w_salary_sum / np.sum(p_valid_given_s)\n","\n","  # estimate std\n","  w_sqdiff_sum = 0\n","  for i in range(N): # Do not need to double loop since N is 1-D\n","    w_sqdiff_sum += p_valid_given_s[i]*(salary[i] - salary_mean)**2\n","  salary_std = np.sqrt(w_sqdiff_sum / np.sum(p_valid_given_s))\n","\n","\n","  # estimate pz\n","  pz = np.mean(p_valid_given_s)\n","  # plot_est(true_salary, salary_mean) we do not know true salary, we can not plot\n","  # print('Std: {:0.3f}'.format(salary_std))\n","\n","  if np.all(np.abs(last_mean-salary_mean)<0.00001): # check for convergence\n","    break\n","\n","# min + max\n","salary_min = salary[p_valid_given_s[:, 0] > 0.5].min()\n","salary_max = salary[p_valid_given_s[:, 0] > 0.5].max()\n","\n","print('Mean: {}  Std: {}  Min: {}   Max: {}'.format(salary_mean, salary_std, salary_min, salary_max))\n","\n","# Print the first five indices of salaries that are not likely to be valid\n","print(np.where(p_valid_given_s < 0.5)[0][:5])"],"metadata":{"id":"lejhsklSqjbL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729395166346,"user_tz":300,"elapsed":308,"user":{"displayName":"Gio Zavalza","userId":"17675791871333725061"}},"outputId":"bc0dc613-bd21-40f4-e615-d5e8727f7fb9"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Salary shape: (200,)\n","\n","iter 0\n","[[0.763 0.773 0.775 0.77  0.779 0.778 0.777 0.759 0.779 0.778 0.769 0.775\n","  0.732 0.771 0.775 0.755 0.778 0.764 0.009 0.767 0.776 0.721 0.778 0.775\n","  0.745 0.775 0.778 0.778 0.044 0.726 0.779 0.779 0.778 0.779 0.779 0.776\n","  0.779 0.771 0.773 0.778 0.778 0.764 0.778 0.722 0.751 0.716 0.769 0.779\n","  0.769 0.    0.776 0.775 0.777 0.779 0.771 0.774 0.775 0.778 0.774 0.779\n","  0.778 0.774 0.704 0.691 0.778 0.74  0.778 0.779 0.763 0.772 0.732 0.777\n","  0.767 0.777 0.778 0.768 0.775 0.738 0.757 0.776 0.779 0.778 0.778 0.777\n","  0.77  0.778 0.763 0.762 0.777 0.774 0.775 0.779 0.774 0.756 0.775 0.774\n","  0.777 0.77  0.776 0.777 0.779 0.779 0.779 0.741 0.757 0.772 0.739 0.775\n","  0.779 0.752 0.744 0.772 0.766 0.777 0.778 0.775 0.778 0.771 0.77  0.727\n","  0.753 0.762 0.778 0.776 0.77  0.754 0.778 0.    0.    0.739 0.778 0.779\n","  0.316 0.768 0.779 0.77  0.771 0.776 0.771 0.779 0.773 0.778 0.761 0.761\n","  0.777 0.779 0.745 0.729 0.773 0.747 0.778 0.771 0.777 0.761 0.779 0.779\n","  0.765 0.006 0.77  0.777 0.778 0.778 0.779 0.778 0.768 0.771 0.759 0.774\n","  0.766 0.765 0.711 0.777 0.767 0.779 0.001 0.772 0.779 0.626 0.773 0.779\n","  0.768 0.771 0.765 0.759 0.757 0.768 0.779 0.77  0.756 0.776 0.769 0.721\n","  0.778 0.777 0.741 0.777 0.769 0.778 0.772 0.763]]\n","\n","iter 1\n","[[0.958 0.967 0.968 0.965 0.962 0.967 0.968 0.952 0.963 0.967 0.964 0.937\n","  0.887 0.966 0.937 0.945 0.967 0.958 0.    0.888 0.943 0.842 0.966 0.968\n","  0.925 0.968 0.966 0.955 0.    0.861 0.961 0.965 0.967 0.966 0.965 0.968\n","  0.966 0.918 0.967 0.959 0.967 0.959 0.967 0.847 0.939 0.814 0.964 0.966\n","  0.964 0.    0.968 0.968 0.968 0.965 0.966 0.968 0.968 0.967 0.967 0.96\n","  0.967 0.967 0.742 0.641 0.956 0.914 0.968 0.964 0.855 0.966 0.887 0.949\n","  0.962 0.968 0.967 0.963 0.968 0.908 0.95  0.968 0.963 0.967 0.966 0.968\n","  0.965 0.967 0.957 0.956 0.968 0.932 0.94  0.961 0.967 0.783 0.968 0.968\n","  0.968 0.965 0.944 0.949 0.963 0.961 0.96  0.915 0.95  0.922 0.909 0.968\n","  0.961 0.94  0.923 0.966 0.961 0.948 0.967 0.968 0.967 0.966 0.965 0.867\n","  0.942 0.957 0.966 0.968 0.964 0.945 0.953 0.    0.    0.909 0.966 0.965\n","  0.    0.963 0.959 0.965 0.966 0.968 0.966 0.964 0.967 0.967 0.831 0.955\n","  0.948 0.964 0.925 0.407 0.928 0.93  0.967 0.965 0.968 0.833 0.96  0.964\n","  0.96  0.    0.909 0.968 0.967 0.967 0.962 0.959 0.896 0.966 0.952 0.934\n","  0.962 0.96  0.785 0.968 0.888 0.961 0.    0.922 0.966 0.004 0.967 0.963\n","  0.963 0.965 0.96  0.952 0.95  0.963 0.964 0.965 0.948 0.942 0.964 0.838\n","  0.958 0.968 0.914 0.952 0.964 0.966 0.966 0.958]]\n","\n","iter 2\n","[[0.988 0.991 0.991 0.99  0.989 0.991 0.991 0.985 0.989 0.991 0.99  0.976\n","  0.953 0.99  0.975 0.983 0.991 0.988 0.    0.943 0.979 0.925 0.99  0.991\n","  0.973 0.991 0.99  0.985 0.    0.937 0.988 0.99  0.991 0.99  0.99  0.991\n","  0.99  0.964 0.991 0.987 0.991 0.988 0.99  0.928 0.98  0.904 0.99  0.99\n","  0.99  0.    0.991 0.991 0.991 0.99  0.991 0.991 0.991 0.991 0.991 0.988\n","  0.991 0.991 0.845 0.745 0.986 0.968 0.991 0.989 0.918 0.991 0.953 0.982\n","  0.989 0.991 0.99  0.99  0.991 0.965 0.984 0.991 0.989 0.991 0.99  0.991\n","  0.99  0.991 0.987 0.987 0.991 0.973 0.977 0.988 0.991 0.85  0.991 0.991\n","  0.991 0.99  0.98  0.982 0.989 0.988 0.987 0.969 0.984 0.967 0.965 0.991\n","  0.988 0.98  0.972 0.991 0.989 0.982 0.991 0.991 0.991 0.991 0.99  0.941\n","  0.981 0.987 0.99  0.991 0.99  0.982 0.984 0.    0.    0.965 0.99  0.99\n","  0.    0.99  0.987 0.99  0.99  0.991 0.991 0.989 0.991 0.991 0.896 0.987\n","  0.982 0.989 0.973 0.369 0.97  0.976 0.991 0.99  0.991 0.898 0.988 0.989\n","  0.989 0.    0.958 0.991 0.991 0.991 0.989 0.987 0.95  0.991 0.986 0.974\n","  0.989 0.988 0.882 0.991 0.944 0.988 0.    0.967 0.99  0.    0.991 0.989\n","  0.989 0.99  0.989 0.985 0.984 0.989 0.989 0.99  0.984 0.978 0.99  0.922\n","  0.986 0.991 0.968 0.984 0.99  0.99  0.991 0.988]]\n","\n","iter 3\n","[[0.992 0.994 0.994 0.994 0.992 0.994 0.994 0.99  0.993 0.994 0.993 0.984\n","  0.97  0.994 0.984 0.989 0.994 0.992 0.    0.962 0.986 0.951 0.993 0.994\n","  0.983 0.994 0.994 0.99  0.    0.959 0.992 0.993 0.994 0.993 0.993 0.994\n","  0.993 0.976 0.994 0.991 0.994 0.992 0.994 0.954 0.987 0.938 0.993 0.993\n","  0.993 0.    0.994 0.994 0.994 0.993 0.994 0.994 0.994 0.994 0.994 0.992\n","  0.994 0.994 0.899 0.828 0.99  0.979 0.994 0.993 0.945 0.994 0.97  0.988\n","  0.993 0.994 0.994 0.993 0.994 0.977 0.99  0.994 0.993 0.994 0.994 0.994\n","  0.994 0.994 0.992 0.991 0.994 0.982 0.985 0.992 0.994 0.899 0.994 0.994\n","  0.994 0.994 0.987 0.988 0.993 0.992 0.992 0.98  0.99  0.978 0.977 0.994\n","  0.992 0.987 0.982 0.994 0.993 0.988 0.994 0.994 0.994 0.994 0.994 0.962\n","  0.988 0.992 0.994 0.994 0.993 0.988 0.99  0.    0.    0.978 0.994 0.993\n","  0.    0.993 0.991 0.993 0.994 0.994 0.994 0.993 0.994 0.994 0.931 0.991\n","  0.988 0.993 0.983 0.489 0.98  0.984 0.994 0.994 0.994 0.932 0.992 0.993\n","  0.992 0.    0.972 0.994 0.994 0.994 0.992 0.991 0.967 0.994 0.99  0.983\n","  0.993 0.992 0.923 0.994 0.963 0.992 0.    0.978 0.993 0.001 0.994 0.992\n","  0.993 0.994 0.992 0.99  0.99  0.993 0.993 0.994 0.989 0.986 0.993 0.95\n","  0.991 0.994 0.979 0.989 0.993 0.993 0.994 0.992]]\n","\n","iter 4\n","[[0.993 0.995 0.995 0.994 0.993 0.995 0.995 0.991 0.993 0.994 0.994 0.986\n","  0.974 0.994 0.986 0.99  0.995 0.993 0.    0.967 0.988 0.958 0.994 0.995\n","  0.985 0.995 0.994 0.991 0.    0.965 0.993 0.994 0.994 0.994 0.994 0.995\n","  0.994 0.979 0.995 0.992 0.995 0.993 0.994 0.96  0.988 0.947 0.994 0.994\n","  0.994 0.    0.995 0.995 0.995 0.994 0.994 0.995 0.995 0.995 0.995 0.993\n","  0.994 0.995 0.913 0.852 0.991 0.982 0.995 0.994 0.953 0.994 0.974 0.99\n","  0.994 0.995 0.994 0.994 0.995 0.98  0.991 0.995 0.993 0.994 0.994 0.995\n","  0.994 0.994 0.993 0.992 0.995 0.984 0.987 0.993 0.995 0.913 0.995 0.995\n","  0.995 0.994 0.988 0.989 0.993 0.993 0.993 0.982 0.991 0.981 0.98  0.995\n","  0.993 0.989 0.984 0.994 0.993 0.989 0.994 0.995 0.994 0.994 0.994 0.967\n","  0.989 0.993 0.994 0.995 0.994 0.99  0.991 0.    0.    0.98  0.994 0.994\n","  0.    0.994 0.992 0.994 0.994 0.995 0.994 0.994 0.995 0.994 0.94  0.992\n","  0.989 0.994 0.985 0.54  0.983 0.986 0.994 0.994 0.995 0.942 0.993 0.994\n","  0.993 0.    0.976 0.995 0.995 0.995 0.993 0.992 0.971 0.994 0.992 0.985\n","  0.994 0.993 0.934 0.995 0.968 0.993 0.    0.981 0.994 0.001 0.995 0.993\n","  0.994 0.994 0.993 0.991 0.991 0.994 0.994 0.994 0.991 0.987 0.994 0.956\n","  0.992 0.995 0.982 0.99  0.994 0.994 0.995 0.993]]\n","\n","iter 5\n","[[0.993 0.995 0.995 0.994 0.993 0.995 0.995 0.992 0.994 0.995 0.994 0.986\n","  0.975 0.995 0.986 0.99  0.995 0.993 0.    0.969 0.988 0.96  0.994 0.995\n","  0.985 0.995 0.994 0.991 0.    0.966 0.993 0.994 0.995 0.994 0.994 0.995\n","  0.994 0.98  0.995 0.992 0.995 0.993 0.994 0.961 0.989 0.949 0.994 0.994\n","  0.994 0.    0.995 0.995 0.995 0.994 0.995 0.995 0.995 0.995 0.995 0.993\n","  0.995 0.995 0.917 0.858 0.992 0.982 0.995 0.994 0.955 0.995 0.975 0.99\n","  0.994 0.995 0.994 0.994 0.995 0.981 0.991 0.995 0.994 0.995 0.994 0.995\n","  0.994 0.995 0.993 0.993 0.995 0.985 0.987 0.993 0.995 0.917 0.995 0.995\n","  0.995 0.994 0.989 0.99  0.994 0.993 0.993 0.983 0.991 0.982 0.981 0.995\n","  0.993 0.989 0.985 0.995 0.994 0.99  0.995 0.995 0.995 0.995 0.994 0.968\n","  0.989 0.993 0.994 0.995 0.994 0.99  0.991 0.    0.    0.981 0.994 0.994\n","  0.    0.994 0.993 0.994 0.995 0.995 0.995 0.994 0.995 0.995 0.943 0.992\n","  0.99  0.994 0.985 0.555 0.983 0.987 0.995 0.994 0.995 0.944 0.993 0.994\n","  0.994 0.    0.977 0.995 0.995 0.995 0.993 0.993 0.972 0.995 0.992 0.985\n","  0.994 0.993 0.937 0.995 0.969 0.993 0.    0.981 0.994 0.001 0.995 0.994\n","  0.994 0.994 0.993 0.992 0.991 0.994 0.994 0.994 0.991 0.988 0.994 0.958\n","  0.992 0.995 0.982 0.991 0.994 0.994 0.995 0.993]]\n","\n","iter 6\n","[[0.993 0.995 0.995 0.994 0.993 0.995 0.995 0.992 0.994 0.995 0.994 0.986\n","  0.975 0.995 0.986 0.99  0.995 0.993 0.    0.969 0.988 0.96  0.994 0.995\n","  0.985 0.995 0.994 0.991 0.    0.967 0.993 0.994 0.995 0.994 0.994 0.995\n","  0.994 0.98  0.995 0.993 0.995 0.993 0.995 0.962 0.989 0.949 0.994 0.994\n","  0.994 0.    0.995 0.995 0.995 0.994 0.995 0.995 0.995 0.995 0.995 0.993\n","  0.995 0.995 0.917 0.86  0.992 0.983 0.995 0.994 0.955 0.995 0.975 0.99\n","  0.994 0.995 0.995 0.994 0.995 0.981 0.991 0.995 0.994 0.995 0.994 0.995\n","  0.994 0.995 0.993 0.993 0.995 0.985 0.987 0.993 0.995 0.918 0.995 0.995\n","  0.995 0.994 0.989 0.99  0.994 0.993 0.993 0.983 0.991 0.982 0.981 0.995\n","  0.993 0.989 0.985 0.995 0.994 0.99  0.995 0.995 0.995 0.995 0.994 0.968\n","  0.99  0.993 0.994 0.995 0.994 0.99  0.991 0.    0.    0.981 0.994 0.994\n","  0.    0.994 0.993 0.994 0.995 0.995 0.995 0.994 0.995 0.995 0.944 0.993\n","  0.99  0.994 0.985 0.559 0.984 0.987 0.995 0.995 0.995 0.945 0.993 0.994\n","  0.994 0.    0.977 0.995 0.995 0.995 0.993 0.993 0.972 0.995 0.992 0.985\n","  0.994 0.993 0.937 0.995 0.969 0.993 0.    0.982 0.994 0.001 0.995 0.994\n","  0.994 0.995 0.994 0.992 0.991 0.994 0.994 0.994 0.991 0.988 0.994 0.959\n","  0.992 0.995 0.983 0.991 0.994 0.994 0.995 0.993]]\n","\n","iter 7\n","[[0.993 0.995 0.995 0.994 0.993 0.995 0.995 0.992 0.994 0.995 0.994 0.986\n","  0.975 0.995 0.986 0.99  0.995 0.993 0.    0.969 0.988 0.96  0.994 0.995\n","  0.986 0.995 0.994 0.991 0.    0.967 0.993 0.994 0.995 0.994 0.994 0.995\n","  0.994 0.98  0.995 0.993 0.995 0.993 0.995 0.962 0.989 0.949 0.994 0.994\n","  0.994 0.    0.995 0.995 0.995 0.994 0.995 0.995 0.995 0.995 0.995 0.993\n","  0.995 0.995 0.918 0.86  0.992 0.983 0.995 0.994 0.955 0.995 0.975 0.99\n","  0.994 0.995 0.995 0.994 0.995 0.981 0.991 0.995 0.994 0.995 0.994 0.995\n","  0.994 0.995 0.993 0.993 0.995 0.985 0.987 0.993 0.995 0.918 0.995 0.995\n","  0.995 0.994 0.989 0.99  0.994 0.993 0.993 0.983 0.991 0.982 0.981 0.995\n","  0.993 0.989 0.985 0.995 0.994 0.99  0.995 0.995 0.995 0.995 0.994 0.969\n","  0.99  0.993 0.994 0.995 0.994 0.99  0.991 0.    0.    0.981 0.994 0.994\n","  0.    0.994 0.993 0.994 0.995 0.995 0.995 0.994 0.995 0.995 0.944 0.993\n","  0.99  0.994 0.985 0.561 0.984 0.987 0.995 0.995 0.995 0.945 0.993 0.994\n","  0.994 0.    0.977 0.995 0.995 0.995 0.993 0.993 0.972 0.995 0.992 0.986\n","  0.994 0.993 0.938 0.995 0.969 0.993 0.    0.982 0.994 0.001 0.995 0.994\n","  0.994 0.995 0.994 0.992 0.991 0.994 0.994 0.994 0.991 0.988 0.994 0.959\n","  0.992 0.995 0.983 0.991 0.994 0.994 0.995 0.993]]\n","\n","iter 8\n","[[0.993 0.995 0.995 0.994 0.993 0.995 0.995 0.992 0.994 0.995 0.994 0.986\n","  0.975 0.995 0.986 0.99  0.995 0.993 0.    0.969 0.988 0.96  0.994 0.995\n","  0.986 0.995 0.994 0.991 0.    0.967 0.993 0.994 0.995 0.994 0.994 0.995\n","  0.994 0.98  0.995 0.993 0.995 0.993 0.995 0.962 0.989 0.949 0.994 0.994\n","  0.994 0.    0.995 0.995 0.995 0.994 0.995 0.995 0.995 0.995 0.995 0.993\n","  0.995 0.995 0.918 0.86  0.992 0.983 0.995 0.994 0.955 0.995 0.975 0.99\n","  0.994 0.995 0.995 0.994 0.995 0.981 0.991 0.995 0.994 0.995 0.994 0.995\n","  0.994 0.995 0.993 0.993 0.995 0.985 0.987 0.993 0.995 0.918 0.995 0.995\n","  0.995 0.994 0.989 0.99  0.994 0.993 0.993 0.983 0.991 0.982 0.981 0.995\n","  0.993 0.989 0.985 0.995 0.994 0.99  0.995 0.995 0.995 0.995 0.994 0.969\n","  0.99  0.993 0.994 0.995 0.994 0.99  0.991 0.    0.    0.981 0.994 0.994\n","  0.    0.994 0.993 0.994 0.995 0.995 0.995 0.994 0.995 0.995 0.944 0.993\n","  0.99  0.994 0.985 0.561 0.984 0.987 0.995 0.995 0.995 0.945 0.993 0.994\n","  0.994 0.    0.977 0.995 0.995 0.995 0.993 0.993 0.973 0.995 0.992 0.986\n","  0.994 0.993 0.938 0.995 0.969 0.993 0.    0.982 0.994 0.001 0.995 0.994\n","  0.994 0.995 0.994 0.992 0.991 0.994 0.994 0.994 0.991 0.988 0.994 0.959\n","  0.992 0.995 0.983 0.991 0.994 0.994 0.995 0.993]]\n","\n","iter 9\n","[[0.993 0.995 0.995 0.994 0.993 0.995 0.995 0.992 0.994 0.995 0.994 0.986\n","  0.975 0.995 0.986 0.99  0.995 0.993 0.    0.969 0.988 0.96  0.994 0.995\n","  0.986 0.995 0.994 0.991 0.    0.967 0.993 0.994 0.995 0.994 0.994 0.995\n","  0.994 0.98  0.995 0.993 0.995 0.993 0.995 0.962 0.989 0.949 0.994 0.994\n","  0.994 0.    0.995 0.995 0.995 0.994 0.995 0.995 0.995 0.995 0.995 0.993\n","  0.995 0.995 0.918 0.86  0.992 0.983 0.995 0.994 0.955 0.995 0.975 0.99\n","  0.994 0.995 0.995 0.994 0.995 0.981 0.991 0.995 0.994 0.995 0.994 0.995\n","  0.994 0.995 0.993 0.993 0.995 0.985 0.987 0.993 0.995 0.918 0.995 0.995\n","  0.995 0.994 0.989 0.99  0.994 0.993 0.993 0.983 0.991 0.982 0.981 0.995\n","  0.993 0.989 0.985 0.995 0.994 0.99  0.995 0.995 0.995 0.995 0.994 0.969\n","  0.99  0.993 0.994 0.995 0.994 0.99  0.991 0.    0.    0.981 0.994 0.994\n","  0.    0.994 0.993 0.994 0.995 0.995 0.995 0.994 0.995 0.995 0.944 0.993\n","  0.99  0.994 0.985 0.561 0.984 0.987 0.995 0.995 0.995 0.945 0.993 0.994\n","  0.994 0.    0.977 0.995 0.995 0.995 0.993 0.993 0.973 0.995 0.992 0.986\n","  0.994 0.993 0.938 0.995 0.969 0.993 0.    0.982 0.994 0.001 0.995 0.994\n","  0.994 0.995 0.994 0.992 0.991 0.994 0.994 0.994 0.991 0.988 0.994 0.959\n","  0.992 0.995 0.983 0.991 0.994 0.994 0.995 0.993]]\n","\n","iter 10\n","[[0.993 0.995 0.995 0.994 0.993 0.995 0.995 0.992 0.994 0.995 0.994 0.986\n","  0.975 0.995 0.986 0.99  0.995 0.993 0.    0.969 0.988 0.96  0.994 0.995\n","  0.986 0.995 0.994 0.991 0.    0.967 0.993 0.994 0.995 0.994 0.994 0.995\n","  0.994 0.98  0.995 0.993 0.995 0.993 0.995 0.962 0.989 0.949 0.994 0.994\n","  0.994 0.    0.995 0.995 0.995 0.994 0.995 0.995 0.995 0.995 0.995 0.993\n","  0.995 0.995 0.918 0.86  0.992 0.983 0.995 0.994 0.955 0.995 0.975 0.99\n","  0.994 0.995 0.995 0.994 0.995 0.981 0.991 0.995 0.994 0.995 0.994 0.995\n","  0.994 0.995 0.993 0.993 0.995 0.985 0.987 0.993 0.995 0.918 0.995 0.995\n","  0.995 0.994 0.989 0.99  0.994 0.993 0.993 0.983 0.991 0.982 0.981 0.995\n","  0.993 0.989 0.985 0.995 0.994 0.99  0.995 0.995 0.995 0.995 0.994 0.969\n","  0.99  0.993 0.994 0.995 0.994 0.99  0.991 0.    0.    0.981 0.994 0.994\n","  0.    0.994 0.993 0.994 0.995 0.995 0.995 0.994 0.995 0.995 0.944 0.993\n","  0.99  0.994 0.985 0.561 0.984 0.987 0.995 0.995 0.995 0.945 0.993 0.994\n","  0.994 0.    0.977 0.995 0.995 0.995 0.993 0.993 0.973 0.995 0.992 0.986\n","  0.994 0.993 0.938 0.995 0.969 0.993 0.    0.982 0.994 0.001 0.995 0.994\n","  0.994 0.995 0.994 0.992 0.991 0.994 0.994 0.994 0.991 0.988 0.994 0.959\n","  0.992 0.995 0.983 0.991 0.994 0.994 0.995 0.993]]\n","\n","iter 11\n","[[0.993 0.995 0.995 0.994 0.993 0.995 0.995 0.992 0.994 0.995 0.994 0.986\n","  0.975 0.995 0.986 0.99  0.995 0.993 0.    0.969 0.988 0.96  0.994 0.995\n","  0.986 0.995 0.994 0.991 0.    0.967 0.993 0.994 0.995 0.994 0.994 0.995\n","  0.994 0.98  0.995 0.993 0.995 0.993 0.995 0.962 0.989 0.949 0.994 0.994\n","  0.994 0.    0.995 0.995 0.995 0.994 0.995 0.995 0.995 0.995 0.995 0.993\n","  0.995 0.995 0.918 0.86  0.992 0.983 0.995 0.994 0.955 0.995 0.975 0.99\n","  0.994 0.995 0.995 0.994 0.995 0.981 0.991 0.995 0.994 0.995 0.994 0.995\n","  0.994 0.995 0.993 0.993 0.995 0.985 0.987 0.993 0.995 0.918 0.995 0.995\n","  0.995 0.994 0.989 0.99  0.994 0.993 0.993 0.983 0.991 0.982 0.981 0.995\n","  0.993 0.989 0.985 0.995 0.994 0.99  0.995 0.995 0.995 0.995 0.994 0.969\n","  0.99  0.993 0.994 0.995 0.994 0.99  0.991 0.    0.    0.981 0.994 0.994\n","  0.    0.994 0.993 0.994 0.995 0.995 0.995 0.994 0.995 0.995 0.944 0.993\n","  0.99  0.994 0.985 0.561 0.984 0.987 0.995 0.995 0.995 0.945 0.993 0.994\n","  0.994 0.    0.977 0.995 0.995 0.995 0.993 0.993 0.973 0.995 0.992 0.986\n","  0.994 0.993 0.938 0.995 0.969 0.993 0.    0.982 0.994 0.001 0.995 0.994\n","  0.994 0.995 0.994 0.992 0.991 0.994 0.994 0.994 0.991 0.988 0.994 0.959\n","  0.992 0.995 0.983 0.991 0.994 0.994 0.995 0.993]]\n","\n","iter 12\n","[[0.993 0.995 0.995 0.994 0.993 0.995 0.995 0.992 0.994 0.995 0.994 0.986\n","  0.975 0.995 0.986 0.99  0.995 0.993 0.    0.969 0.988 0.96  0.994 0.995\n","  0.986 0.995 0.994 0.991 0.    0.967 0.993 0.994 0.995 0.994 0.994 0.995\n","  0.994 0.98  0.995 0.993 0.995 0.993 0.995 0.962 0.989 0.949 0.994 0.994\n","  0.994 0.    0.995 0.995 0.995 0.994 0.995 0.995 0.995 0.995 0.995 0.993\n","  0.995 0.995 0.918 0.86  0.992 0.983 0.995 0.994 0.955 0.995 0.975 0.99\n","  0.994 0.995 0.995 0.994 0.995 0.981 0.991 0.995 0.994 0.995 0.994 0.995\n","  0.994 0.995 0.993 0.993 0.995 0.985 0.987 0.993 0.995 0.918 0.995 0.995\n","  0.995 0.994 0.989 0.99  0.994 0.993 0.993 0.983 0.991 0.982 0.981 0.995\n","  0.993 0.989 0.985 0.995 0.994 0.99  0.995 0.995 0.995 0.995 0.994 0.969\n","  0.99  0.993 0.994 0.995 0.994 0.99  0.991 0.    0.    0.981 0.994 0.994\n","  0.    0.994 0.993 0.994 0.995 0.995 0.995 0.994 0.995 0.995 0.944 0.993\n","  0.99  0.994 0.985 0.561 0.984 0.987 0.995 0.995 0.995 0.945 0.993 0.994\n","  0.994 0.    0.977 0.995 0.995 0.995 0.993 0.993 0.973 0.995 0.992 0.986\n","  0.994 0.993 0.938 0.995 0.969 0.993 0.    0.982 0.994 0.001 0.995 0.994\n","  0.994 0.995 0.994 0.992 0.991 0.994 0.994 0.994 0.991 0.988 0.994 0.959\n","  0.992 0.995 0.983 0.991 0.994 0.994 0.995 0.993]]\n","\n","iter 13\n","[[0.993 0.995 0.995 0.994 0.993 0.995 0.995 0.992 0.994 0.995 0.994 0.986\n","  0.975 0.995 0.986 0.99  0.995 0.993 0.    0.969 0.988 0.96  0.994 0.995\n","  0.986 0.995 0.994 0.991 0.    0.967 0.993 0.994 0.995 0.994 0.994 0.995\n","  0.994 0.98  0.995 0.993 0.995 0.993 0.995 0.962 0.989 0.949 0.994 0.994\n","  0.994 0.    0.995 0.995 0.995 0.994 0.995 0.995 0.995 0.995 0.995 0.993\n","  0.995 0.995 0.918 0.86  0.992 0.983 0.995 0.994 0.955 0.995 0.975 0.99\n","  0.994 0.995 0.995 0.994 0.995 0.981 0.991 0.995 0.994 0.995 0.994 0.995\n","  0.994 0.995 0.993 0.993 0.995 0.985 0.987 0.993 0.995 0.918 0.995 0.995\n","  0.995 0.994 0.989 0.99  0.994 0.993 0.993 0.983 0.991 0.982 0.981 0.995\n","  0.993 0.989 0.985 0.995 0.994 0.99  0.995 0.995 0.995 0.995 0.994 0.969\n","  0.99  0.993 0.994 0.995 0.994 0.99  0.991 0.    0.    0.981 0.994 0.994\n","  0.    0.994 0.993 0.994 0.995 0.995 0.995 0.994 0.995 0.995 0.944 0.993\n","  0.99  0.994 0.985 0.561 0.984 0.987 0.995 0.995 0.995 0.945 0.993 0.994\n","  0.994 0.    0.977 0.995 0.995 0.995 0.993 0.993 0.973 0.995 0.992 0.986\n","  0.994 0.993 0.938 0.995 0.969 0.993 0.    0.982 0.994 0.001 0.995 0.994\n","  0.994 0.995 0.994 0.992 0.991 0.994 0.994 0.994 0.991 0.988 0.994 0.959\n","  0.992 0.995 0.983 0.991 0.994 0.994 0.995 0.993]]\n","\n","iter 14\n","[[0.993 0.995 0.995 0.994 0.993 0.995 0.995 0.992 0.994 0.995 0.994 0.986\n","  0.975 0.995 0.986 0.99  0.995 0.993 0.    0.969 0.988 0.96  0.994 0.995\n","  0.986 0.995 0.994 0.991 0.    0.967 0.993 0.994 0.995 0.994 0.994 0.995\n","  0.994 0.98  0.995 0.993 0.995 0.993 0.995 0.962 0.989 0.949 0.994 0.994\n","  0.994 0.    0.995 0.995 0.995 0.994 0.995 0.995 0.995 0.995 0.995 0.993\n","  0.995 0.995 0.918 0.86  0.992 0.983 0.995 0.994 0.955 0.995 0.975 0.99\n","  0.994 0.995 0.995 0.994 0.995 0.981 0.991 0.995 0.994 0.995 0.994 0.995\n","  0.994 0.995 0.993 0.993 0.995 0.985 0.987 0.993 0.995 0.918 0.995 0.995\n","  0.995 0.994 0.989 0.99  0.994 0.993 0.993 0.983 0.991 0.982 0.981 0.995\n","  0.993 0.989 0.985 0.995 0.994 0.99  0.995 0.995 0.995 0.995 0.994 0.969\n","  0.99  0.993 0.994 0.995 0.994 0.99  0.991 0.    0.    0.981 0.994 0.994\n","  0.    0.994 0.993 0.994 0.995 0.995 0.995 0.994 0.995 0.995 0.944 0.993\n","  0.99  0.994 0.985 0.561 0.984 0.987 0.995 0.995 0.995 0.945 0.993 0.994\n","  0.994 0.    0.977 0.995 0.995 0.995 0.993 0.993 0.973 0.995 0.992 0.986\n","  0.994 0.993 0.938 0.995 0.969 0.993 0.    0.982 0.994 0.001 0.995 0.994\n","  0.994 0.995 0.994 0.992 0.991 0.994 0.994 0.994 0.991 0.988 0.994 0.959\n","  0.992 0.995 0.983 0.991 0.994 0.994 0.995 0.993]]\n","\n","iter 15\n","[[0.993 0.995 0.995 0.994 0.993 0.995 0.995 0.992 0.994 0.995 0.994 0.986\n","  0.975 0.995 0.986 0.99  0.995 0.993 0.    0.969 0.988 0.96  0.994 0.995\n","  0.986 0.995 0.994 0.991 0.    0.967 0.993 0.994 0.995 0.994 0.994 0.995\n","  0.994 0.98  0.995 0.993 0.995 0.993 0.995 0.962 0.989 0.949 0.994 0.994\n","  0.994 0.    0.995 0.995 0.995 0.994 0.995 0.995 0.995 0.995 0.995 0.993\n","  0.995 0.995 0.918 0.86  0.992 0.983 0.995 0.994 0.955 0.995 0.975 0.99\n","  0.994 0.995 0.995 0.994 0.995 0.981 0.991 0.995 0.994 0.995 0.994 0.995\n","  0.994 0.995 0.993 0.993 0.995 0.985 0.987 0.993 0.995 0.918 0.995 0.995\n","  0.995 0.994 0.989 0.99  0.994 0.993 0.993 0.983 0.991 0.982 0.981 0.995\n","  0.993 0.989 0.985 0.995 0.994 0.99  0.995 0.995 0.995 0.995 0.994 0.969\n","  0.99  0.993 0.994 0.995 0.994 0.99  0.991 0.    0.    0.981 0.994 0.994\n","  0.    0.994 0.993 0.994 0.995 0.995 0.995 0.994 0.995 0.995 0.944 0.993\n","  0.99  0.994 0.985 0.561 0.984 0.987 0.995 0.995 0.995 0.945 0.993 0.994\n","  0.994 0.    0.977 0.995 0.995 0.995 0.993 0.993 0.973 0.995 0.992 0.986\n","  0.994 0.993 0.938 0.995 0.969 0.993 0.    0.982 0.994 0.001 0.995 0.994\n","  0.994 0.995 0.994 0.992 0.991 0.994 0.994 0.994 0.991 0.988 0.994 0.959\n","  0.992 0.995 0.983 0.991 0.994 0.994 0.995 0.993]]\n","Mean: [111984.38462802]  Std: [17966.36278974]  Min: 64694.0   Max: 169008.0\n","[ 18  28  49 127 128]\n"]}]},{"cell_type":"markdown","source":["## Part 3: Stretch Goals\n","Include all your code used for any stretch goals in this section. Add headings where appropriate."],"metadata":{"id":"3X3j_efPhh6e"}},{"cell_type":"code","source":["# TO DO (optional)\n"],"metadata":{"id":"UFZfuCqJqhLc","executionInfo":{"status":"ok","timestamp":1729395166346,"user_tz":300,"elapsed":5,"user":{"displayName":"Gio Zavalza","userId":"17675791871333725061"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# from https://gist.github.com/jonathanagustin/b67b97ef12c53a8dec27b343dca4abba\n","# install can take a minute\n","\n","import os\n","# @title Convert Notebook to PDF. Save Notebook to given directory\n","NOTEBOOKS_DIR = \"/content/drive/My Drive/CS441/24FA/hw3\" # @param {type:\"string\"}\n","NOTEBOOK_NAME = \"CS441_FA24_HW3_Solution.ipynb\" # @param {type:\"string\"}\n","#------------------------------------------------------------------------------#\n","from google.colab import drive\n","drive.mount(\"/content/drive/\", force_remount=True)\n","NOTEBOOK_PATH = f\"{NOTEBOOKS_DIR}/{NOTEBOOK_NAME}\"\n","assert os.path.exists(NOTEBOOK_PATH), f\"NOTEBOOK NOT FOUND: {NOTEBOOK_PATH}\"\n","!apt install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic > /dev/null 2>&1\n","!jupyter nbconvert \"$NOTEBOOK_PATH\" --to pdf > /dev/null 2>&1\n","NOTEBOOK_PDF = NOTEBOOK_PATH.rsplit('.', 1)[0] + '.pdf'\n","assert os.path.exists(NOTEBOOK_PDF), f\"ERROR MAKING PDF: {NOTEBOOK_PDF}\"\n","print(f\"PDF CREATED: {NOTEBOOK_PDF}\")"],"metadata":{"id":"5T2IRr7fz6ta","executionInfo":{"status":"error","timestamp":1729395184379,"user_tz":300,"elapsed":18037,"user":{"displayName":"Gio Zavalza","userId":"17675791871333725061"}},"colab":{"base_uri":"https://localhost:8080/","height":211},"outputId":"a1b975b1-fe4f-49fe-a33f-f50f8c32cb50"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]},{"output_type":"error","ename":"AssertionError","evalue":"ERROR MAKING PDF: /content/drive/My Drive/CS441/24FA/hw3/CS441_FA24_HW3_Solution.pdf","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-2e445fcf8ef6>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'jupyter nbconvert \"$NOTEBOOK_PATH\" --to pdf > /dev/null 2>&1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mNOTEBOOK_PDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOTEBOOK_PATH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pdf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNOTEBOOK_PDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"ERROR MAKING PDF: {NOTEBOOK_PDF}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"PDF CREATED: {NOTEBOOK_PDF}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: ERROR MAKING PDF: /content/drive/My Drive/CS441/24FA/hw3/CS441_FA24_HW3_Solution.pdf"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1wD57teWXN3bnwMgLs6wm8MONgh1Xq58v","timestamp":1729297417248},{"file_id":"10Nj5-SuSQV0-72rL7-InC9lVPhmYhwVA","timestamp":1729296715206},{"file_id":"1J0B6_dcu9qjy6hqHOvfK7TQY3SNAtD84","timestamp":1707174089980},{"file_id":"16NPS0eAXdrha1CvrTipxKhNRXIQRghda","timestamp":1705680043685},{"file_id":"1EyDprvfSjGmR5oM4k0IFTPYLEFzQIpAz","timestamp":1673299853915},{"file_id":"1-roWT29Q7bvNPuwnejXmlMBhymBQnKfm","timestamp":1673043755861}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}